{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# VLM Erax","metadata":{}},{"cell_type":"code","source":"!python -m pip install -q git+https://github.com/huggingface/transformers accelerate\n!python -m pip install -q qwen-vl-utils\n!pip install flash-attn -q --no-build-isolation","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T03:56:33.402796Z","iopub.execute_input":"2025-02-17T03:56:33.403067Z","iopub.status.idle":"2025-02-17T03:57:33.322254Z","shell.execute_reply.started":"2025-02-17T03:56:33.403046Z","shell.execute_reply":"2025-02-17T03:57:33.321416Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.7/38.7 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install -qU gdown\n!gdown --id 1ZNWz-exIqUm-EJqNQM2_-6Wb_Br0XGhj\n!unzip /kaggle/working/samples.zip -d /kaggle/working","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T03:57:34.013157Z","iopub.execute_input":"2025-02-17T03:57:34.013459Z","iopub.status.idle":"2025-02-17T03:57:44.167819Z","shell.execute_reply.started":"2025-02-17T03:57:34.013431Z","shell.execute_reply":"2025-02-17T03:57:44.166709Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom (original): https://drive.google.com/uc?id=1ZNWz-exIqUm-EJqNQM2_-6Wb_Br0XGhj\nFrom (redirected): https://drive.google.com/uc?id=1ZNWz-exIqUm-EJqNQM2_-6Wb_Br0XGhj&confirm=t&uuid=09786d21-94ae-4b12-a8a5-64be8ac7e010\nTo: /kaggle/working/samples.zip\n100%|██████████████████████████████████████| 27.9M/27.9M [00:00<00:00, 28.3MB/s]\nArchive:  /kaggle/working/samples.zip\n   creating: /kaggle/working/samples/\n   creating: /kaggle/working/samples/idcard/\n  inflating: /kaggle/working/samples/idcard/0fb04a58-c484-42ab-96bd-f8d8750e4c55_091074011629_IDFront_jpg.rf.2732aa3ab85d162eeb596fc910025edb.jpg  \n  inflating: /kaggle/working/samples/idcard/0fb04a58-c484-42ab-96bd-f8d8750e4c55_091074011629_IDFront_jpg.rf.dc5365dacf34bff5605c25b218f4f02a.jpg  \n  inflating: /kaggle/working/samples/idcard/0fb04a58-c484-42ab-96bd-f8d8750e4c55_091074011629_IDFront_jpg.rf.feedc88c44e78568f98508dcaac3857a.jpg  \n  inflating: /kaggle/working/samples/idcard/0fc8fb2c-c323-401b-946a-d76df63cdffb_024096009391_IDFront_jpg.rf.1501dd3f2e51a5043d034907281b89bd.jpg  \n  inflating: /kaggle/working/samples/idcard/0fc8fb2c-c323-401b-946a-d76df63cdffb_024096009391_IDFront_jpg.rf.a7a259504e4755b104d424f1421357d5.jpg  \n  inflating: /kaggle/working/samples/idcard/0_jpg.rf.6c3ba6fef26ebc2fb8e288cb30a9db9e.jpg  \n  inflating: /kaggle/working/samples/idcard/0_jpg.rf.c9c8aa007495722c6f95001af4ffedcb.jpg  \n  inflating: /kaggle/working/samples/idcard/1_jpg.rf.3fea043008c12c5b45146886df8e3d8c.jpg  \n  inflating: /kaggle/working/samples/idcard/1_jpg.rf.93e2b9423547f28196d1cee6eab7b26e.jpg  \n  inflating: /kaggle/working/samples/idcard/1_jpg.rf.94a0ec52a13c2d6c34971ada922bcc0c.jpg  \n  inflating: /kaggle/working/samples/idcard/1_jpg.rf.b5df1b388ffed929873a16b3f2dbf089.jpg  \n  inflating: /kaggle/working/samples/idcard/2_jpg.rf.12803ed18c299f06439d1c9510bcc591.jpg  \n  inflating: /kaggle/working/samples/idcard/2_jpg.rf.170085a9de12027dea18eda6fb904bb8.jpg  \n  inflating: /kaggle/working/samples/idcard/2_jpg.rf.2e3e711059e46f1944ac422a77976b00.jpg  \n  inflating: /kaggle/working/samples/idcard/2_jpg.rf.3afd2e196c5681da93891dee4d665d67.jpg  \n  inflating: /kaggle/working/samples/idcard/2_jpg.rf.73eeba39bad8fabb70432638cb0071a5.jpg  \n  inflating: /kaggle/working/samples/idcard/2_jpg.rf.76ac3f578da3fe88869fef1077bb64b0.jpg  \n  inflating: /kaggle/working/samples/idcard/2_jpg.rf.8294f6a3b45a573ac042c65a1de17d94.jpg  \n  inflating: /kaggle/working/samples/idcard/2_jpg.rf.a9c60b7966d430bba5125027bfb623a1.jpg  \n  inflating: /kaggle/working/samples/idcard/2_jpg.rf.b6f194d26b0e12eff99d00c54a8f627d.jpg  \n  inflating: /kaggle/working/samples/idcard/2_jpg.rf.b9610e45c5db380fbcec4d22397e2fd0.jpg  \n  inflating: /kaggle/working/samples/idcard/2_jpg.rf.e5e030e93b16caf5db5ec4386b2b64b8.jpg  \n  inflating: /kaggle/working/samples/idcard/2_jpg.rf.fad57ec76f51ad395808df7b9a6b18d4.jpg  \n  inflating: /kaggle/working/samples/idcard/3_jpg.rf.21707bb58fa8826b21fb964ecd365374.jpg  \n  inflating: /kaggle/working/samples/idcard/3_jpg.rf.29f5082a5461dc7692053365c0a8a61e.jpg  \n  inflating: /kaggle/working/samples/idcard/3_jpg.rf.3782ed518db4debbffc2b7872399f362.jpg  \n  inflating: /kaggle/working/samples/idcard/3_jpg.rf.5132d59bd31ec8f8bce39cc60c9774ba.jpg  \n  inflating: /kaggle/working/samples/idcard/3_jpg.rf.61e5599dc781ca58157605aaf1a7b233.jpg  \n  inflating: /kaggle/working/samples/idcard/3_jpg.rf.7cf826f01d6591bffd753536bf0e63a5.jpg  \n  inflating: /kaggle/working/samples/idcard/3_jpg.rf.7f38f61ef0acb9d0d60dd85582eb67d0.jpg  \n  inflating: /kaggle/working/samples/idcard/3_jpg.rf.857857f22ce00843c182e1438aa97112.jpg  \n  inflating: /kaggle/working/samples/idcard/3_jpg.rf.97f7558aebcc78f1ebb2993c065562c1.jpg  \n  inflating: /kaggle/working/samples/idcard/3_jpg.rf.b765710ac9e79e5d53a70f7a13b164dd.jpg  \n  inflating: /kaggle/working/samples/idcard/3_jpg.rf.c07c29def77df46ad5090745f3bf73cc.jpg  \n  inflating: /kaggle/working/samples/idcard/3_jpg.rf.c3a322c2e3d77b675c2e90b4603534b4.jpg  \n  inflating: /kaggle/working/samples/idcard/3_jpg.rf.c4e77fb968ea09b4e0a06eb9d48450a1.jpg  \n  inflating: /kaggle/working/samples/idcard/3_jpg.rf.d34f3b1355b25aa7e59cda388190c619.jpg  \n  inflating: /kaggle/working/samples/idcard/3_jpg.rf.ea15ce4873a547d3b3d932724e9dd36e.jpg  \n  inflating: /kaggle/working/samples/idcard/3_jpg.rf.f41f3bede4d64d9ffbfa9d0f1c2a5ed5.jpg  \n  inflating: /kaggle/working/samples/idcard/4_jpg.rf.1c36b2eab1fe7cae62fd099fc67f31aa.jpg  \n  inflating: /kaggle/working/samples/idcard/4_jpg.rf.1eda79cbd643517d70d9ea4f78c15d40.jpg  \n  inflating: /kaggle/working/samples/idcard/4_jpg.rf.27ba42bb5f6cf42a605a51674b5eac80.jpg  \n  inflating: /kaggle/working/samples/idcard/4_jpg.rf.2ff632667ac67d5170092a477bc0094a.jpg  \n  inflating: /kaggle/working/samples/idcard/4_jpg.rf.560179efa48ce4cae59fc41c7f94e7e3.jpg  \n  inflating: /kaggle/working/samples/idcard/4_jpg.rf.5c16426ad794fee9f625cc7ecf4d104d.jpg  \n  inflating: /kaggle/working/samples/idcard/4_jpg.rf.84096af1ccc1b0e98dff761ba801d9a3.jpg  \n  inflating: /kaggle/working/samples/idcard/4_jpg.rf.878576fae7487bf4b083355fa1dc74c5.jpg  \n  inflating: /kaggle/working/samples/idcard/4_jpg.rf.9081a99bc521556c15392d0f5a774086.jpg  \n  inflating: /kaggle/working/samples/idcard/4_jpg.rf.9813bc2e3518fb3e5b8516911dce5e52.jpg  \n  inflating: /kaggle/working/samples/idcard/4_jpg.rf.99fb88f022bfb9164daf4a2a0aae0f53.jpg  \n  inflating: /kaggle/working/samples/idcard/4_jpg.rf.aa5953eb016f067d178ce0f2a4a7078f.jpg  \n  inflating: /kaggle/working/samples/idcard/4_jpg.rf.b6c1b20859841e3eb2111e0084581b4e.jpg  \n  inflating: /kaggle/working/samples/idcard/4_jpg.rf.df3ea062c6df4777dd337e568ddd4e51.jpg  \n  inflating: /kaggle/working/samples/idcard/4_jpg.rf.f223011435271ddf109d3f95e2eb9b56.jpg  \n  inflating: /kaggle/working/samples/idcard/5_jpg.rf.283b4e86700d914e86ac3e1266f32e7b.jpg  \n  inflating: /kaggle/working/samples/idcard/5_jpg.rf.65c44f24b036f938401a9bfaaf2c54b7.jpg  \n   creating: /kaggle/working/samples/inspection/\n  inflating: /kaggle/working/samples/inspection/045A2AD3-0201-4A66-8C6E-8DD27A33C5B7_6624-110.jpg  \n  inflating: /kaggle/working/samples/inspection/08B8AAEC-814E-4AFC-881F-0B361BB1AD85_6624-79.jpeg  \n  inflating: /kaggle/working/samples/inspection/08B8AAEC-814E-4AFC-881F-0B361BB1AD85_6624-79_rotated_0.jpeg  \n  inflating: /kaggle/working/samples/inspection/08EBCAAF-A969-4975-956F-90ECD235B263_6624-87.jpg  \n  inflating: /kaggle/working/samples/inspection/0BD367BD-18C4-4E03-9744-B665A7F4DCED_6624-31.jpg  \n  inflating: /kaggle/working/samples/inspection/12FFCF5E-85B5-480D-ADDE-EF569917350E_170624-13.jpg  \n  inflating: /kaggle/working/samples/inspection/1E955FC8-D8F1-4953-85DB-9F5E7296EF54_6624-125.png  \n  inflating: /kaggle/working/samples/inspection/2C855E04-01DE-476B-A011-028E95EE917D_6624-180.jpg  \n  inflating: /kaggle/working/samples/inspection/2E4EE234-3A7E-4A40-8D7C-F12166ABEA76_6624-264.jpg  \n  inflating: /kaggle/working/samples/inspection/373FFB79-B198-48CA-9AB3-7B7A5823E2F3_6624-283.jpeg  \n  inflating: /kaggle/working/samples/inspection/39EDE734-470C-4046-BB32-EC50BCA42116_6624-134.jpg  \n  inflating: /kaggle/working/samples/inspection/3C61F088-4829-4BD0-AD9E-A4919BB7FA47_6624-69.jpg  \n  inflating: /kaggle/working/samples/inspection/415A5DE3-B108-4BD0-AAA5-F10B13D114B5_6624-299.jpg  \n  inflating: /kaggle/working/samples/inspection/485AFB74-E5AB-4FD1-A16C-CFD7AA134C86_6624-12.jpeg  \n  inflating: /kaggle/working/samples/inspection/48EC2F5C-D613-4A0E-9F87-3FFA912F5B66_6624-39.jpg  \n  inflating: /kaggle/working/samples/inspection/4B371F53-05E9-459F-99EA-A1554DFFF17C_6624-68.jpg  \n  inflating: /kaggle/working/samples/inspection/4E882C1D-5056-463E-A5AA-8D47A8F47CDF_6624-207.jpg  \n  inflating: /kaggle/working/samples/inspection/56C4D76F-AB44-4479-83E5-96CCB2FFD776_6624-129.jpg  \n  inflating: /kaggle/working/samples/inspection/5E5EE6BB-880F-4904-9515-DA6D459DB8A4_6624-196.jpeg  \n  inflating: /kaggle/working/samples/inspection/5E601DDA-CF07-4E76-967B-D5CE8776B23F_6624-40.jpeg  \n  inflating: /kaggle/working/samples/inspection/6AD8035D-8C3A-4E42-A0B1-B4275639BACD_6624-123.jpg  \n  inflating: /kaggle/working/samples/inspection/6EB0BB90-E264-4DA5-8887-4CE2E1F900F6_6624-7.jpeg  \n  inflating: /kaggle/working/samples/inspection/70CBDA44-5CDB-4B7F-A6D3-6CEA8D75B8E4_170624-146.jpg  \n  inflating: /kaggle/working/samples/inspection/7EBD37C3-9F0F-4A9D-8444-6B84A02B9DFF_6624-18.jpg  \n  inflating: /kaggle/working/samples/inspection/7EBFC216-AA3D-4DEB-8755-4CA6FF99C040_6624-224.jpg  \n  inflating: /kaggle/working/samples/inspection/83F65A92-0F81-4FEB-8658-FAF4151D7C18_6624-13.jpg  \n  inflating: /kaggle/working/samples/inspection/94DAB1C4-7B99-4D73-B015-AB97A55FE92D_6624-270.jpg  \n  inflating: /kaggle/working/samples/inspection/9E6AAAA4-142E-4EF1-8E06-0EC27841DBC6_170624-95.jpg  \n   creating: /kaggle/working/samples/registration/\n  inflating: /kaggle/working/samples/registration/0A7AB05A-C272-4878-B0D8-EA90D97120FE_6624-7_mau3ms.jpeg  \n  inflating: /kaggle/working/samples/registration/0B14CC87-4A2C-44E2-950A-FCCDAF47D31F_6624-163_mau4ms.jpg  \n  inflating: /kaggle/working/samples/registration/0D3C5F00-9595-4438-AC59-DF3D6AF1404F_6624-79_mau3ms.jpg  \n  inflating: /kaggle/working/samples/registration/10_IMG_1635304474491_1635305744775_mau3ms.jpg  \n  inflating: /kaggle/working/samples/registration/11df998acdc1caec16bbf41489371f74_mau4ms.jpg  \n  inflating: /kaggle/working/samples/registration/1C38C44A-8C47-4A42-9D90-43C92D98C8DE_6624-299_mau3ms.jpg  \n  inflating: /kaggle/working/samples/registration/1CBD0897-09EC-4280-B22B-932B1C9E4C08_6624-90_mau4ms.jpg  \n  inflating: /kaggle/working/samples/registration/1E2DCB25-3F8F-4413-B59E-9D027D294FE2_6624-86_mau3ms.jpeg  \n  inflating: /kaggle/working/samples/registration/2CA2643E-F971-41DB-A207-16698E1D92EB_6624-128_mau4ms.jpg  \n  inflating: /kaggle/working/samples/registration/3E1E3DFD-394C-471A-9283-DBEB369B17D6_6624-247_mau3ms.jpg  \n  inflating: /kaggle/working/samples/registration/3EAFAE6D-9D7D-4C0A-AEB4-43248DF91041_170624-113_mau3ms.jpg  \n  inflating: /kaggle/working/samples/registration/3F83C236-2F6F-4B04-A175-8C8DBEF20EBA_6624-251_mau3ms.jpeg  \n  inflating: /kaggle/working/samples/registration/3_324137D5F063-6DB3-4657-A5AC-36CA28A7D1BB_mau4ms.jpg  \n  inflating: /kaggle/working/samples/registration/4A20124A-83A8-413A-BE11-0A2C1202BBB2_mau3ms.jpg  \n  inflating: /kaggle/working/samples/registration/4_24576061-50B0-492E-97AC-B32D63F4CD03_mau3mt.jpg  \n  inflating: /kaggle/working/samples/registration/5A247ECB-FB54-47E1-819D-AF32D7610A0F_mau4mt.jpg  \n  inflating: /kaggle/working/samples/registration/5BF486CA-C8A1-4E9F-82EE-64F97AA4276D_mau3ms.jpg  \n  inflating: /kaggle/working/samples/registration/5EB7350E-1590-4106-8941-47D2251FE7CB_mau3ms.jpg  \n  inflating: /kaggle/working/samples/registration/6_IMG_20211027_101414_mau3ms.jpg  \n  inflating: /kaggle/working/samples/registration/7D718C55-A4DA-4AAB-8A43-50873759DD25_mau3mt.jpg  \n  inflating: /kaggle/working/samples/registration/7FCAFE8E-E612-4D0D-99EA-DF20E4C99CE8_6624-20_mau4ms.jpg  \n  inflating: /kaggle/working/samples/registration/8C67D3D4-C653-4816-B295-9E3836DC227F_6624-42_mau3ms.jpg  \n  inflating: /kaggle/working/samples/registration/8_Screenshot_2021-10-26-14-43-44-010_com.intsig.camscanner~2_mau3mt.jpg  \n  inflating: /kaggle/working/samples/registration/9CA832E9-75BD-434E-A6DB-1A21DA284A1E_6624-203_mau3ms.jpeg  \n  inflating: /kaggle/working/samples/registration/9ECC4E0B-C997-4D81-8BFA-A2F3D395B61C_6624-136_mau3ms.jpg  \n  inflating: /kaggle/working/samples/registration/9ED98696-7DAF-4C2B-A50E-226A74C1D718_170624-7_mau3ms.jpg  \n  inflating: /kaggle/working/samples/registration/9F035FD4-D53E-4251-BBEB-BA282630E40B_6624-1_mau3ms.jpeg  \n  inflating: /kaggle/working/samples/registration/9_Screenshot_2021-10-26-13-15-03-238_com.intsig.camscanner~2_mau3ms.jpg  \n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport base64\nimport json\n\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as  plt\n\nimport torch\nfrom transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\n\nmodel_path = \"erax-ai/EraX-VL-7B-V2.0-Preview\"\n\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(\n    model_path,\n    torch_dtype=torch.bfloat16,\n    attn_implementation=\"eager\", # replace with \"flash_attention_2\" if your GPU is Ampere architecture\n    device_map=\"auto\",\n    # offload_state_dict=True\n)\n\ntokenizer =  AutoTokenizer.from_pretrained(model_path)\n# processor = AutoProcessor.from_pretrained(model_path)\n\nmin_pixels = 256 * 28 * 28\nmax_pixels = 1280 * 28 * 28\nprocessor = AutoProcessor.from_pretrained(\n     model_path,\n     min_pixels=min_pixels,\n     max_pixels=max_pixels,\n )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T03:58:37.690278Z","iopub.execute_input":"2025-02-17T03:58:37.690648Z","iopub.status.idle":"2025-02-17T04:06:29.336695Z","shell.execute_reply.started":"2025-02-17T03:58:37.690607Z","shell.execute_reply":"2025-02-17T04:06:29.336005Z"},"scrolled":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.12k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00b70f4afb054d53b12872b073f02066"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/56.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7aa080e07d544a2d8257d5e8350c9467"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3df9cf9a15174087bce751059bacc74d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1d9405c132e41de81cb4e40d069d8b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4e72bbd1e4f405883631982d5897454"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2034a5367552419185111149ff0fe8c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.69G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7ecf02029674233a904a0a72c4eb111"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"403b9f25b5f442e9bcc8826d4d976fee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/266 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9539d6f0933483d818f25d4c84b2ecc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/4.38k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e89aca24c3ba488b912bd13065848a9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6f10f7aca144c3f8dda1c4c0fc38231"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ffda6d186fe94f828d415637d6a68d86"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f663ba764a62417dbba136e7d87f8232"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/392 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41b0905b316c44aa98b88248aefaaf98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/613 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ed882173d4f47308f61e76892d73c16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/569 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5dc9b2706d7640d8bb3b8d06267bd757"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"chat_template.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"234972bcecc2489ea34ea47c01ebae68"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"image_path = \"/kaggle/working/samples/idcard/0_jpg.rf.6c3ba6fef26ebc2fb8e288cb30a9db9e.jpg\"\n\nwith open(image_path, \"rb\") as f:\n    encoded_image = base64.b64encode(f.read())\ndecoded_image_text = encoded_image.decode('utf-8')\nbase64_data = f\"data:image;base64,{decoded_image_text}\"\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"image\",\n                \"image\": base64_data,\n            },\n            {\n                \"type\": \"text\",\n                \"text\": \"Trích xuất thông tin nội dung từ hình ảnh được cung cấp.\"\n            },\n        ],\n    }\n]\n\n# Prepare prompt\ntokenized_text = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\n\nimage_inputs, video_inputs = process_vision_info(messages)\n\ninputs = processor(\n    text=[ tokenized_text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n\n# Generation configs\ngeneration_config =  model.generation_config\ngeneration_config.do_sample   = True\ngeneration_config.temperature = 0.01\ngeneration_config.top_k       = 1\ngeneration_config.top_p       = 0.001\n#generation_config.min_p       = 0.1\ngeneration_config.best_of     = 1\ngeneration_config.max_new_tokens     = 2048\ngeneration_config.repetition_penalty = 1.01","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T04:32:53.541990Z","iopub.execute_input":"2025-02-17T04:32:53.542401Z","iopub.status.idle":"2025-02-17T04:32:53.608410Z","shell.execute_reply.started":"2025-02-17T04:32:53.542370Z","shell.execute_reply":"2025-02-17T04:32:53.607480Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"%%time\ngenerated_ids = model.generate(**inputs, generation_config=generation_config)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\n\nprint(output_text[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T04:32:55.647825Z","iopub.execute_input":"2025-02-17T04:32:55.648121Z","iopub.status.idle":"2025-02-17T04:33:21.811652Z","shell.execute_reply.started":"2025-02-17T04:32:55.648097Z","shell.execute_reply":"2025-02-17T04:33:21.810925Z"}},"outputs":[{"name":"stdout","text":"Hình ảnh là một thẻ căn cước công dân của Việt Nam. Ở góc trên bên trái là quốc huy Việt Nam, bên phải là mã QR. Phía trên cùng là dòng chữ \"CỘNG HÒA XÃ HỘI CHỦ NGHĨA VIỆT NAM\", bên dưới là dòng chữ \"Độc lập - Tự do - Hạnh phúc\" và dòng chữ tiếng An \"SOCIAL REPUBLIC OF VIET NAM\" và \"Independence - Freedom - Happiness\". Tiếp theo là dòng chữ \"CĂN CƯỚC CÔNG DÂN\" và \"Citizen Identity Card\". Ở giữa là thông tin cá nhân gồm: Số / No.: 320495186074, Họ và tên / Full name: VU THỊ BÍCH, Ngày sinh / Date of birth: 30/04/1991, Giới tính / Sex: Nữ, Quốc tịch / Nationality: Việt Nam, Quê quán / Place of origin: Vĩnh Bảo, Hải Phòng, Nơi thường trú / Place of residence: 17/9/2, Trần Nguyễn Hân, Vĩnh Bảo, Hải Phòng. Ở góc dưới bên trái là dòng chữ \"Có giá trị đến: 30/04/2041\" và \"Date of expiry\".\nCPU times: user 26.2 s, sys: 0 ns, total: 26.2 s\nWall time: 26.2 s\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"# QA Agents","metadata":{}},{"cell_type":"code","source":"!pip install -q ollama gradio\n!curl -fsSL https://ollama.com/install.sh | sh","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T11:04:00.227432Z","iopub.execute_input":"2025-02-19T11:04:00.227621Z","iopub.status.idle":"2025-02-19T11:04:50.643141Z","shell.execute_reply.started":"2025-02-19T11:04:00.227604Z","shell.execute_reply":"2025-02-19T11:04:50.642269Z"},"scrolled":true},"outputs":[{"name":"stderr","text":"    return future.result()\n  File \"<ipython-input-1-3938b4666513>\", line 30, in start_ollama_serve\n  File \"<ipython-input-1-3938b4666513>\", line 9, in run_process\n  File \"/usr/lib/python3.10/asyncio/subprocess.py\", line 218, in create_subprocess_exec\n    transport, protocol = await loop.subprocess_exec(\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1681, in subprocess_exec\n    transport = await self._make_subprocess_transport(\n  File \"/usr/lib/python3.10/asyncio/unix_events.py\", line 207, in _make_subprocess_transport\n    transp = _UnixSubprocessTransport(self, protocol, args, shell,\n  File \"/usr/lib/python3.10/asyncio/base_subprocess.py\", line 36, in __init__\n    self._start(args=args, shell=shell, stdin=stdin, stdout=stdout,\n  File \"/usr/lib/python3.10/asyncio/unix_events.py\", line 799, in _start\n    self._proc = subprocess.Popen(\n  File \"/usr/lib/python3.10/subprocess.py\", line 971, in __init__\n    self._execute_child(args, executable, preexec_fn, close_fds,\n  File \"/usr/lib/python3.10/subprocess.py\", line 1863, in _execute_child\n    raise child_exception_type(errno_num, err_msg, err_filename)\nFileNotFoundError: [Errno 2] No such file or directory: 'ollama'\n","output_type":"stream"},{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.2/62.2 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.9/321.9 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m100.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h>>> Installing ollama to /usr/local\n>>> Downloading Linux amd64 bundle\n############################################################################################# 100.0%\n>>> Creating ollama user...\n>>> Adding ollama user to video group...\n>>> Adding current user to ollama group...\n>>> Creating ollama systemd service...\n\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n>>> The Ollama API is now available at 127.0.0.1:11434.\n>>> Install complete. Run \"ollama\" from the command line.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import asyncio\nimport threading\nimport socket\n\n\ndef run_ollama_server():\n    async def run_process(cmd):\n        print('>>> starting', *cmd)\n        process = await asyncio.create_subprocess_exec(\n            *cmd,\n            stdout=asyncio.subprocess.PIPE,\n            stderr=asyncio.subprocess.PIPE,\n            # env={**os.environ, 'OLLAMA_NUM_PARALLEL': '8', 'OLLAMA_MAX_LOADED_MODELS': '1'}\n        )\n\n        # define an async pipe function\n        async def pipe(lines):\n            async for line in lines:\n                print(line.decode().strip())\n\n            await asyncio.gather(\n                pipe(process.stdout),\n                pipe(process.stderr),\n            )\n\n        # call it\n        await asyncio.gather(pipe(process.stdout), pipe(process.stderr))\n\n    async def start_ollama_serve():\n        await run_process(['ollama', 'serve'])\n\n    def run_async_in_thread(loop, coro):\n        asyncio.set_event_loop(loop)\n        loop.run_until_complete(coro)\n        loop.close()\n\n    # Create a new event loop that will run in a new thread\n    new_loop = asyncio.new_event_loop()\n\n    # Start ollama serve in a separate thread so the cell won't block execution\n    thread = threading.Thread(target=run_async_in_thread, args=(new_loop, start_ollama_serve()))\n    thread.start()\nrun_ollama_server()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T11:04:50.644155Z","iopub.execute_input":"2025-02-19T11:04:50.644473Z","iopub.status.idle":"2025-02-19T11:04:50.655715Z","shell.execute_reply.started":"2025-02-19T11:04:50.644440Z","shell.execute_reply":"2025-02-19T11:04:50.654215Z"},"scrolled":true,"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":">>> starting ollama serve\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import ollama\n# ollama.pull('phi4:14b-q8_0') 16GB RAM + 8GBx2 GPU\n# ollama.pull(\"llama3.1:8b-instruct-fp16\")\nollama.pull(\"llama3.2:3b-instruct-q4_0\")\n# ollama.pull(\"deepseek-r1:32b-qwen-distill-q8_0\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T11:04:50.659748Z","iopub.execute_input":"2025-02-19T11:04:50.660060Z","iopub.status.idle":"2025-02-19T11:05:09.048495Z","shell.execute_reply.started":"2025-02-19T11:04:50.660033Z","shell.execute_reply":"2025-02-19T11:05:09.047738Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"Couldn't find '/root/.ollama/id_ed25519'. Generating new private key.\nYour new public key is:\n\nssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIF4U0h/CEXSo8aPin/kMGYZAi/ANNCunX0FUKQStL2En\n\n2025/02/19 11:04:50 routes.go:1186: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\ntime=2025-02-19T11:04:50.681Z level=INFO source=images.go:432 msg=\"total blobs: 0\"\ntime=2025-02-19T11:04:50.681Z level=INFO source=images.go:439 msg=\"total unused blobs removed: 0\"\ntime=2025-02-19T11:04:50.681Z level=INFO source=routes.go:1237 msg=\"Listening on 127.0.0.1:11434 (version 0.5.11)\"\ntime=2025-02-19T11:04:50.682Z level=INFO source=gpu.go:217 msg=\"looking for compatible GPUs\"\ntime=2025-02-19T11:04:51.059Z level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-78edfed5-cd59-4ca2-ee84-890004b97d04 library=cuda variant=v12 compute=7.5 driver=12.6 name=\"Tesla T4\" total=\"14.7 GiB\" available=\"14.6 GiB\"\ntime=2025-02-19T11:04:51.059Z level=INFO source=types.go:130 msg=\"inference compute\" id=GPU-55c9cd12-b5ac-0077-a34e-d02dd31b8bde library=cuda variant=v12 compute=7.5 driver=12.6 name=\"Tesla T4\" total=\"14.7 GiB\" available=\"14.6 GiB\"\ntime=2025-02-19T11:04:52.162Z level=INFO source=download.go:176 msg=\"downloading 9c8a9ab5edab in 16 119 MB part(s)\"\ntime=2025-02-19T11:04:56.321Z level=INFO source=download.go:176 msg=\"downloading 966de95ca8a6 in 1 1.4 KB part(s)\"\ntime=2025-02-19T11:04:57.495Z level=INFO source=download.go:176 msg=\"downloading fcc5a6bec9da in 1 7.7 KB part(s)\"\ntime=2025-02-19T11:04:58.686Z level=INFO source=download.go:176 msg=\"downloading a70ff7e570d9 in 1 6.0 KB part(s)\"\ntime=2025-02-19T11:04:59.846Z level=INFO source=download.go:176 msg=\"downloading 56bb8bd477a5 in 1 96 B part(s)\"\ntime=2025-02-19T11:05:01.021Z level=INFO source=download.go:176 msg=\"downloading c5b3569010e2 in 1 559 B part(s)\"\n[GIN] 2025/02/19 - 11:05:09 | 200 | 17.410420144s |       127.0.0.1 | POST     \"/api/pull\"","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"ProgressResponse(status='success', completed=None, total=None, digest=None)"},"metadata":{}},{"name":"stdout","text":"\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"### Chatbot","metadata":{}},{"cell_type":"code","source":"LLM_NAME=\"llama3.2:3b-instruct-q4_0\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gradio as gr\nimport ollama\n\n\ndef chat_with_model(message, history):\n    # Format lịch sử chat theo chuẩn Ollama\n    formatted_history = []\n    for human, ai in history[-10:]:\n        formatted_history.append({\"role\": \"user\", \"content\": human})\n        formatted_history.append({\"role\": \"assistant\", \"content\": ai})\n    formatted_history.append({\"role\": \"user\", \"content\": message})\n\n\n    # Tạo stream response từ model\n    response = ollama.chat(\n        # model=\"llama3.1:8b-instruct-fp16\",\n        model=LLM_NAME,\n        messages=formatted_history,\n        stream=True\n    )\n\n    full_response = \"\"\n    for chunk in response:\n        part = chunk[\"message\"][\"content\"]\n        full_response += part\n        yield full_response\n\n# Tạo giao diện Gradio\ndemo = gr.ChatInterface(\n    fn=chat_with_model,\n    title=\"DeepSeek-R1 Chatbot\",\n    examples=[\"Giải thích về AI\", \"Viết một bài thơ ngắn\", \"Cách làm bánh flan?\"],\n    theme=\"soft\"\n).launch(share=True, show_error=True)","metadata":{"trusted":true,"scrolled":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Voicebot","metadata":{}},{"cell_type":"code","source":"from ollama import Client, AsyncClient\n\n# --------------------- CONSTANT ---------------------\nVOICE_API_URL = \"http://127.0.0.1:5000/api/voice\"\nOLLAMA_CLIENT = AsyncClient(\n    host='http://localhost:11434',\n    headers={'x-some-header': 'some-value'}\n)\nLLM_NAME = \"llama3.2:3b-instruct-q4_0\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T11:05:09.049210Z","iopub.execute_input":"2025-02-19T11:05:09.049488Z","iopub.status.idle":"2025-02-19T11:05:09.102755Z","shell.execute_reply.started":"2025-02-19T11:05:09.049465Z","shell.execute_reply":"2025-02-19T11:05:09.102171Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import io\nimport os\nimport tempfile\nimport numpy as np\nimport soundfile as sf\nfrom fastapi import FastAPI, File, UploadFile, HTTPException\nfrom fastapi.responses import JSONResponse\nfrom transformers import pipeline\nfrom typing import Optional\nimport uvicorn\nimport requests\nimport gradio as gr\nfrom typing import List, Generator, AsyncGenerator\n\n# --------------------- Phần 1: Xử lý âm thanh ---------------------\n# Tạo pipeline chuyển đổi giọng nói thành văn bản (ASR) sử dụng mô hình \"vinai/PhoWhisper-small\"\n# Mô hình này được xây dựng dựa trên ý tưởng của Whisper (OpenAI, \"Whisper: Robust Speech Recognition via Large-Scale Weak Supervision\")\n# Dữ liệu âm thanh được biểu diễn dưới dạng np.array, ví dụ: np.array([...], dtype=float32) với shape (n_samples,) hoặc (n_samples, n_channels)\n# Khởi tạo mô hình Whisper sử dụng CPU (device=-1) để tránh lỗi CUDA\ntranscriber = pipeline(\n    \"automatic-speech-recognition\",\n    model=\"vinai/PhoWhisper-large\",\n    device=\"cuda\"\n)\n\napp = FastAPI()\n\ndef speech_to_text_whisper(audio: np.ndarray, sample_rate: int = 16000) -> str:\n    result = transcriber({\"sampling_rate\": sample_rate, \"raw\": audio}, chunk_length_s=30)\n    return result.get(\"text\", \"\")\n\n@app.post(\"/api/voice\")\nasync def voice_api(file: Optional[UploadFile] = File(None)):\n    if file is None or file.filename == \"\":\n        raise HTTPException(status_code=400, detail=\"Không nhận được file âm thanh\")\n\n    temp_file_path = \"\"\n    try:\n        # Tạo file tạm với hậu tố (suffix) phù hợp với định dạng của file upload\n        suffix = os.path.splitext(file.filename)[1]\n        with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:\n            temp_file_path = tmp.name\n            contents = await file.read()\n            tmp.write(contents)\n        # Lấy đường dẫn tuyệt đối của file tạm\n        abs_path = os.path.abspath(temp_file_path)\n        # Đọc file âm thanh từ đường dẫn tuyệt đối\n        audio_data, sr = sf.read(abs_path, dtype='float32')\n        if audio_data.dtype == np.int16:\n            audio_data = audio_data.astype(np.float32) / 32768.0\n        text = speech_to_text_whisper(audio_data, sample_rate=sr)\n        return JSONResponse(content={\"text\": text, \"file_path\": abs_path}, status_code=200)\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=f\"Lỗi xử lý: {str(e)}\")\n    finally:\n        if temp_file_path and os.path.exists(temp_file_path):\n            os.remove(temp_file_path)\n\n# --------------------- Phần 1: Chatbot tích hợp ---------------------\nasync def chat_with_model(message: str, history: List[List[str]]) -> AsyncGenerator[List[List[str]], None]:\n    \"\"\"\n    Gọi mô hình chat (Ollama) với lịch sử hội thoại hiện tại và tin nhắn mới.\n    Input:\n      - message: tin nhắn của người dùng (str)\n      - history: danh sách lịch sử chat, mỗi phần tử [tin nhắn người dùng, phản hồi AI]\n    Output:\n      - Generator trả về lịch sử chat được cập nhật theo từng phần của phản hồi.\n    \"\"\"\n    # Chuẩn hóa lịch sử chat theo định dạng của Ollama\n    formatted_history = []\n    for human, ai in history[-10:]:\n        formatted_history.append({\"role\": \"user\", \"content\": human})\n        formatted_history.append({\"role\": \"assistant\", \"content\": ai})\n    formatted_history.append({\"role\": \"user\", \"content\": message})\n\n    # # Gọi hàm chat của Ollama với stream=True để nhận phản hồi từng phần\n    # response = ollama.chat(\n    #     model=\"llama3.2:3b-instruct-q4_0\",\n    #     messages=formatted_history,\n    #     stream=True\n    # )\n    response = await OLLAMA_CLIENT.chat(\n        model=LLM_NAME,\n        messages=formatted_history,\n        stream=True\n    )\n\n    full_response = \"\"\n    async for chunk in response:\n        part = chunk[\"message\"][\"content\"]\n        full_response += part\n        # Cập nhật lịch sử chat theo thời gian thực\n        updated_history = history + [[message, full_response]]\n        yield updated_history\n\n\ndef send_audio_to_api(audio_file_path: str) -> str:\n    \"\"\"\n    Gửi file âm thanh đến API FastAPI để chuyển đổi thành văn bản.\n    Input:\n      - audio_file_path: đường dẫn file âm thanh (str)\n    Output:\n      - Văn bản chuyển đổi (str) hoặc thông báo lỗi.\n    \"\"\"\n    if not audio_file_path:\n        return \"\"\n    try:\n        with open(audio_file_path, \"rb\") as file:\n            files = {\"file\": (audio_file_path.split(\"/\")\n                              [-1], file, \"audio/wav\")}\n            response = requests.post(VOICE_API_URL, files=files)\n            response.raise_for_status()\n            data = response.json()\n            return data.get(\"text\", \"\")\n    except requests.exceptions.RequestException as e:\n        print(f\"Lỗi gửi audio đến API: {e}\")\n        return f\"Lỗi: {e}\"\n    except Exception as e:\n        print(f\"Lỗi không xác định: {e}\")\n        return f\"Lỗi: {e}\"\n\n\n# --------------------- Phần 2: Giao diện Gradio ---------------------\nwith gr.Blocks() as demo:\n    chatbot = gr.Chatbot(label=\"Voicebot demo\")\n    examples = gr.Examples(\n        examples=[\n            \"Giải thích về AI\",\n            \"Viết một bài thơ ngắn\",\n            \"Cách làm bánh flan?\"\n        ],\n        inputs=[gr.Textbox(label=\"Ví dụ\", visible=False)]\n    )\n\n    with gr.Row():\n        # Cho phép nhập âm thanh từ microphone, trả về đường dẫn file tạm thời\n        audio_input = gr.Audio(\n            sources=[\"microphone\"], type=\"filepath\", label=\"Input audio (optional)\")\n        text_output_from_audio = gr.Textbox(label=\"Text recognized\", interactive=False)\n\n    # Khi có sự thay đổi từ input âm thanh, gửi file đến API để chuyển đổi\n    audio_input.change(fn=send_audio_to_api, inputs=audio_input,\n                       outputs=text_output_from_audio)\n\n    async def process_input(message: str, history: List[List[str]], transcribed_text: str) -> AsyncGenerator[List[List[str]], None]:\n        \"\"\"\n        Xử lý đầu vào: ưu tiên dùng văn bản chuyển từ âm thanh nếu có, ngược lại dùng tin nhắn nhập tay.\n        Gọi hàm chat_with_model để nhận phản hồi từng phần từ mô hình.\n        \"\"\"\n        user_input = transcribed_text if transcribed_text else message\n        response_generator = chat_with_model(user_input, history)\n        async for updated_history in response_generator:\n            yield updated_history, None, None, None\n\n    text_input = gr.Textbox(label=\"Input text...\")\n    submit_btn = gr.Button(\"Submit\")\n    submit_btn.click(fn=process_input, inputs=[\n                     text_input, chatbot, text_output_from_audio],\n                     outputs=[chatbot, text_input, text_output_from_audio, audio_input])\n\n# --------------------- Phần 3: Chạy FastAPI và Gradio song song ---------------------\ndef run_fastapi():\n    uvicorn.run(app, host=\"127.0.0.1\", port=5000)\n\n\n# --------------------- Chạy FastAPI trên một luồng riêng để không làm gián đoạn Gradio ---------------------\nfastapi_thread = threading.Thread(target=run_fastapi, daemon=True)\nfastapi_thread.start()\ndemo.launch(share=True, server_port=8000, show_error=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-19T11:05:09.103562Z","iopub.execute_input":"2025-02-19T11:05:09.103817Z","execution_failed":"2025-02-19T11:45:11.113Z"},"scrolled":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.33k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccd7c994eaf248f4b999a676e8e55947"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/6.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b6ff5277fc6411298867ebf0241b0a7"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}